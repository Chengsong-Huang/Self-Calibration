{
    "hf_token": "your_hf_token",
    "distributed": true,
    "use_wandb": true,
    "wandb_project": "multidataset",
    "total_train_samples": 100000,
    "total_eval_samples": 5000,
    "batch_size": 1,
    "gradient_accumulation_steps": 64,
    "num_epochs": 1,
    "learning_rate": 5e-5,
    "model_name": "meta-llama/Llama-3.1-8B-Instruct",
    "lora_r": 32,
    "lora_alpha": 16,
    "lora_dropout": 0.05,
    "causal_lm_ratio": 0.7,
    "use_zero": false,
    "dataset": "HINT-lab/Llama_3.1-8B-Instruct-Self-Calibration",
    "threshold": 0.75,
    "datasets": [
      {
        "name": "gsm8k",
        "train_percentage": 0.15,
        "eval_percentage": 0.15
      },
      {
        "name": "svamp",
        "train_percentage": 0.15,
        "eval_percentage": 0.15
      },
      {
        "name": "sciq",
        "train_percentage": 0.05,
        "eval_percentage": 0.05
      },
      {
        "name": "commonsense_qa",
        "train_percentage": 0.05,
        "eval_percentage": 0.05
      },
      {
        "name": "winogrande",
        "train_percentage": 0.05,
        "eval_percentage": 0.05
      },
      {
        "name": "openbookqa",
        "train_percentage": 0.05,
        "eval_percentage": 0.05
      },
      {
        "name": "reclor",
        "train_percentage": 0.05,
        "eval_percentage": 0.05
      },
      {
        "name": "arc_easy",
        "train_percentage": 0.05,
        "eval_percentage": 0.05
      },
      {
        "name": "logiqa",
        "train_percentage": 0.05,
        "eval_percentage": 0.05
      }
    ],
    "output_dir": "./loras/llama"
  }
  